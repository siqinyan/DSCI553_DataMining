{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "\n",
    "'''\n",
    "    Task (12.5pts)\n",
    "    \n",
    "    Task description\n",
    "    You will write the K-Means and Bradley-Fayyad-Reina (BFR) algorithms from scratch. You should implement K-Means as the in-memory clustering algorithm that you will use in BFR. You will iteratively load the data points from a file and process these data points with the BFR algorithm. See below pseudocode for your reference.\n",
    "    \n",
    "    for file in input_path:\n",
    "        data_points = load(file)\n",
    "        if first round:\n",
    "            run K-means for initialization\n",
    "        else:\n",
    "            run BFR(data_points)\n",
    "    In BFR, there are three sets of points that you need to keep track of: Discard set (DS), Compression set (CS), Retained set (RS). For each cluster in the DS and CS, the cluster is summarized by:\n",
    "        - N: The number of points\n",
    "        - SUM: the sum of the coordinates of the points\n",
    "        - SUMSQ: the sum of squares of coordinates\n",
    "'''\n",
    "\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import copy\n",
    "import csv\n",
    "import os\n",
    "import itertools\n",
    "from collections import defaultdict\n",
    "from pyspark import SparkContext\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calc_distance(A, B, std=None, distance_type='euclidean'):\n",
    "    if distance_type=='euclidean':\n",
    "        return float(math.sqrt(sum([(a-b)**2 for a,b in zip(A, B)])))\n",
    "    elif distance_type=='mahalanobis':\n",
    "        return float(math.sqrt(sum([((a-b)/sd)**2 for a,b,sd in zip(A, B, std)])))\n",
    "\n",
    "def calc_col_avg(prev, cur, denominator=None):\n",
    "    tmp_list = []\n",
    "    tmp_list.append(prev)\n",
    "    tmp_list.append(cur)\n",
    "    if denominator is None:\n",
    "        return [sum(i)/len(i) for i in zip(*tmp_list)]\n",
    "    else:\n",
    "        return [sum(i)/denominator for i in zip(*tmp_list)]\n",
    "\n",
    "def export_file(data, output_file, output_type='json', export_type='w+'):\n",
    "    if output_type=='json':\n",
    "        with open(output_file, export_type) as f:\n",
    "            f.writelines(json.dumps(data))\n",
    "    elif output_type=='csv':\n",
    "        with open(output_file, export_type, newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for k,v in data.items():\n",
    "                writer.writerow(v)\n",
    "\n",
    "                \n",
    "                \n",
    "class KMeans():\n",
    "    def __init__(self, n_cluster, max_iter=300, seed=666):\n",
    "        self.n_cluster = n_cluster\n",
    "        self.max_iter = max_iter\n",
    "        self.seed = seed\n",
    "        self.centroid_info = {}\n",
    "        self.centroid_stat_info = {}\n",
    "        self.clusters_points = {}\n",
    "        self.stable = {}\n",
    "        self.data_dict = None\n",
    "\n",
    "    \n",
    "    def _check_data(self):\n",
    "        if len(self.data_dict) < self.n_cluster:\n",
    "            self.n_cluster = len(self.data_dict)\n",
    "        \n",
    "    def _init_centroids(self):\n",
    "        random.seed(self.seed)\n",
    "        for i,k in enumerate(random.sample(self.data_dict.keys(), self.n_cluster)):\n",
    "            self.centroid_info.setdefault('c'+str(i), self.data_dict.get(k))\n",
    "            self.centroid_stat_info.setdefault('c'+str(i), self.data_dict.get(k))\n",
    "            self.clusters_points.setdefault('c'+str(i), list())\n",
    "            self.stable.setdefault('c'+str(i), False)\n",
    "    \n",
    "    def _update_centroids(self):\n",
    "        info_prev = copy.deepcopy(self.centroid_info)\n",
    "        for c, ps in self.clusters_points.items():\n",
    "            if not self.stable.get(c):\n",
    "                tmp_list = []\n",
    "                tmp_list.append(self.centroid_info.get(c))\n",
    "                for p in ps:\n",
    "                    tmp_list.append(self.data_dict.get(p))\n",
    "\n",
    "                self.centroid_info[c] = [sum(i) / len(i) for i in zip(*tmp_list)]\n",
    "                self.centroid_stat_info[c] = [sum([v**2 for v in i]) / len(i) for i in zip(*tmp_list)]\n",
    "\n",
    "        return info_prev, self.centroid_info\n",
    "        \n",
    "        \n",
    "    def fit(self, data_dict):\n",
    "        self.data_dict = data_dict\n",
    "        self._check_data()\n",
    "        self._init_centroids()\n",
    "        n_iter = 1\n",
    "        while True: # can not be while n_iter<self.max_iter, will _clear()\n",
    "            for k in self.data_dict:\n",
    "                tmp_dict = {}\n",
    "                for c in self.centroid_info:\n",
    "                    tmp_dict[(c, k)] = calc_distance(self.centroid_info[c], self.data_dict[k])\n",
    "                tmp_info = list(sorted(tmp_dict.items(), key=lambda kv: kv[1]))[:1]\n",
    "                self.clusters_points[tmp_info[0][0][0]].append(tmp_info[0][0][1])\n",
    "\n",
    "            info_prev, info_cur = self._update_centroids()\n",
    "            if not self._updated(info_prev, info_cur) or n_iter>=self.max_iter:\n",
    "                break\n",
    "            self._clear()\n",
    "            n_iter += 1\n",
    "\n",
    "        return self.centroid_info, self.centroid_stat_info, self.clusters_points\n",
    "\n",
    "    \n",
    "    def _clear(self):\n",
    "        for c in self.clusters_points:\n",
    "            self.clusters_points[c] = []\n",
    "\n",
    "    def _updated(self, prev, cur):\n",
    "        for k in prev:\n",
    "            value_prev = set(map(lambda v: round(v, 0), prev.get(k)))\n",
    "            value_cur = set(map(lambda v: round(v, 0), cur.get(k)))\n",
    "            if len(value_prev.difference(value_cur))==0:\n",
    "                self.stable[k] = True\n",
    "            else:\n",
    "                self.stable[k] = False\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "class Cluster():\n",
    "    def __init__(self):\n",
    "        self.SUM_N = None\n",
    "        self.SUMSQ_N = None\n",
    "        self.clusters_points = None\n",
    "        self.signature = None\n",
    "\n",
    "    def init(self, info, stat, clusters_points):\n",
    "        self.SUM_N = info\n",
    "        self.SUMSQ_N = stat\n",
    "        self.clusters_points = clusters_points\n",
    "        self.n_point = 0\n",
    "        self.n_dim = len(list(info.values())[0])\n",
    "        self.STD = dict()\n",
    "        self.calc_std()\n",
    "        \n",
    "\n",
    "    def calc_n_point(self):\n",
    "        self.n_point = 0\n",
    "        for _,v in self.clusters_points.items():\n",
    "            if type(v)==list:\n",
    "                self.n_point += len(v)\n",
    "        return self.n_point\n",
    "                \n",
    "    def calc_n_cluster(self):\n",
    "        return len(self.SUM_N)\n",
    "        \n",
    "    def calc_std(self):\n",
    "        self.STD = {}\n",
    "        for k in self.SUM_N:\n",
    "            self.STD[k] = [math.sqrt(sq_n - sum_n**2) \\\n",
    "                               for sq_n, sum_n in zip(self.SUMSQ_N.get(k), self.SUM_N.get(k))]\n",
    "\n",
    "        \n",
    "    def update_centroid_info(self, tmp_clusters_points, tmp_data_dict):\n",
    "        if len(tmp_clusters_points)>0:\n",
    "            sum_n_prev = copy.deepcopy(self.SUM_N)\n",
    "            sumsq_n_prev = copy.deepcopy(self.SUMSQ_N)\n",
    "            clusters_points_prev = copy.deepcopy(self.clusters_points)\n",
    "            for c, ps in tmp_clusters_points.items():\n",
    "                tmp_list = []\n",
    "                loc_prev1 = sum_n_prev.get(c)\n",
    "                len_cluster_prev = len(clusters_points_prev.get(c))\n",
    "                loc_prev1 = list(map(lambda v: v*len_cluster_prev, loc_prev1))\n",
    "                for p in ps:\n",
    "                    tmp_list.append(tmp_data_dict.get(p))\n",
    "\n",
    "                loc_prev2 = [sum(i) for i in zip(*tmp_list)]\n",
    "                len_cluster_new = len(tmp_list) + len_cluster_prev\n",
    "                self.SUM_N[c] = calc_col_avg(loc_prev1, loc_prev2, denominator=len_cluster_new)\n",
    "\n",
    "                loc_prev3 = sumsq_n_prev.get(c)\n",
    "                loc_prev3 = list(map(lambda v: v*len_cluster_prev, loc_prev3))\n",
    "                loc_prev4 = [sum([v**2 for v in i]) for i in zip(*tmp_list)]\n",
    "                self.SUMSQ_N[c] = calc_col_avg(loc_prev3, loc_prev4, denominator=len_cluster_new)\n",
    "\n",
    "            self.calc_std()\n",
    "            self.update_clusters_points(tmp_clusters_points)\n",
    "\n",
    "    def update_clusters_points(self, tmp_clusters_points):\n",
    "        if len(tmp_clusters_points)>0:\n",
    "            combined = defaultdict(list)\n",
    "            for k, v in itertools.chain(self.clusters_points.items(), tmp_clusters_points.items()):\n",
    "                combined[k] += v\n",
    "                \n",
    "            self.clusters_points = combined\n",
    "            \n",
    "            \n",
    "    def get_points_of_cluster(self, k):\n",
    "        return list(self.clusters_points.get(k))\n",
    "    \n",
    "    def get_centroid_of_clustuer(self, k):\n",
    "        return list(self.SUM_N.get(k))\n",
    "    \n",
    "    def get_std_of_clustuer(self, k):\n",
    "        return list(self.STD.get(k))\n",
    "    \n",
    "    def get_sumsq_of_clustuer(self, k):\n",
    "        return list(self.SUMSQ_N.get(k))\n",
    "    \n",
    "\n",
    "class DS(Cluster):\n",
    "    def __init__(self):\n",
    "        Cluster.__init__(self)\n",
    "        self.signature = 'DS'\n",
    "\n",
    "    def merge(self, ds_clusters, cs_cluster_sumsq, cs_cluster_centroid, cs_clusters_points):\n",
    "        loc_prev1 = self.get_centroid_of_clustuer(ds_clusters)\n",
    "        len_prev = len(self.get_points_of_cluster(ds_clusters))\n",
    "        loc_prev1 = list(map(lambda v: v*len_prev, loc_prev1))\n",
    "\n",
    "        len_cur = len(cs_clusters_points)\n",
    "        loc_prev2 = list(map(lambda v: v*len_cur, cs_cluster_centroid))\n",
    "        loc_cur = calc_col_avg(loc_prev1,\n",
    "                               loc_prev2,\n",
    "                               denominator=len_prev+len_cur)\n",
    "\n",
    "        sumsq_prev1 = list(map(lambda v: v*len_prev, self.get_sumsq_of_clustuer(ds_clusters)))\n",
    "        sumsq_prev2 = list(map(lambda v: v*len_cur, cs_cluster_sumsq))\n",
    "        sumsq_cur = calc_col_avg(sumsq_prev1,\n",
    "                                 sumsq_prev2,\n",
    "                                 denominator=len_prev+len_cur)\n",
    "\n",
    "        self.SUM_N.update({ds_clusters: loc_cur})\n",
    "        self.clusters_points[ds_clusters].extend(cs_clusters_points)\n",
    "        self.SUMSQ_N.update({ds_clusters: sumsq_cur})\n",
    "\n",
    "        self.calc_n_point()\n",
    "        self.calc_std()\n",
    "\n",
    "\n",
    "class CS(Cluster):\n",
    "    def __init__(self):\n",
    "        Cluster.__init__(self)\n",
    "        self.signature = 'CS'\n",
    "        self.r2c_index = 0\n",
    "        self.merge_index = 0\n",
    "\n",
    "    def remove_cluster(self, c):\n",
    "        self.SUM_N.pop(c)\n",
    "        self.SUMSQ_N.pop(c)\n",
    "        self.clusters_points.pop(c)\n",
    "        self.STD.pop(c)\n",
    "        self.calc_n_point()\n",
    "\n",
    "    def delta_update(self, info, stat, clusters_points):\n",
    "        if len(info)!=0:\n",
    "            for k in info:\n",
    "                self.SUM_N.update({'r2c'+ str(self.r2c_index): info.get(k)})\n",
    "                self.SUMSQ_N.update({'r2c'+ str(self.r2c_index): stat.get(k)})\n",
    "                self.clusters_points.update({'r2c'+ str(self.r2c_index): clusters_points.get(k)})\n",
    "                self.calc_std()\n",
    "                self.r2c_index += 1\n",
    "\n",
    "    def merge(self, cluster1, cluster2):\n",
    "        loc_cur = calc_col_avg(list(self.SUM_N[cluster1]), list(self.SUM_N[cluster2]))\n",
    "        sumsq_cur = calc_col_avg(list(self.SUMSQ_N[cluster1]), list(self.SUMSQ_N[cluster2]))\n",
    "        clusters_points_cur = list(self.clusters_points[cluster1])\n",
    "        clusters_points_cur.extend(list(self.clusters_points[cluster2]))\n",
    "\n",
    "        cluster_cur = 'm'+str(self.merge_index)\n",
    "        self.SUM_N.pop(cluster1)\n",
    "        self.SUM_N.pop(cluster2)\n",
    "        self.SUM_N.update({cluster_cur: loc_cur})\n",
    "\n",
    "        self.SUMSQ_N.pop(cluster1)\n",
    "        self.SUMSQ_N.pop(cluster2)\n",
    "        self.SUMSQ_N.update({cluster_cur: sumsq_cur})\n",
    "\n",
    "        self.clusters_points.pop(cluster1)\n",
    "        self.clusters_points.pop(cluster2)\n",
    "        self.clusters_points.update({cluster_cur: clusters_points_cur})\n",
    "\n",
    "        self.calc_std()\n",
    "        self.merge_index += 1\n",
    "\n",
    "    def sort(self):\n",
    "        result = defaultdict(list)\n",
    "        for c in self.clusters_points:\n",
    "            result[c] = sorted(self.clusters_points[c])\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "class RS():\n",
    "    def __init__(self):\n",
    "        self.remaining_set = {}\n",
    "\n",
    "    def add(self, data):\n",
    "        self.remaining_set.update(data)\n",
    "        \n",
    "    def gather(self, left):\n",
    "        self.remaining_set = left\n",
    "        \n",
    "    def calc_n_point(self):\n",
    "        return len(self.remaining_set)\n",
    "\n",
    "    @classmethod\n",
    "    def getSignature(cls):\n",
    "        return 'RS'\n",
    "\n",
    "    \n",
    "class IntermediateRecords():\n",
    "    def __init__(self):\n",
    "        self.intermediate_records = {}\n",
    "        self.intermediate_records[\"header\"] = (\"round_id\", \n",
    "                                               \"nof_cluster_discard\", \"nof_point_discard\",\n",
    "                                               \"nof_cluster_compression\", \"nof_point_compression\",\n",
    "                                               \"nof_point_retained\")\n",
    "\n",
    "    def save_check_point(self, round_id, DS, CS, RS):\n",
    "        self.intermediate_records[round_id] = (round_id,\n",
    "                                               DS.calc_n_cluster(), DS.calc_n_point(),\n",
    "                                               CS.calc_n_cluster(), CS.calc_n_point(),\n",
    "                                               RS.calc_n_point())\n",
    "        print(\"{} -> DS_INFO: C:{} NUM:{} | CS_INFO: C:{} NUM:{} | RS_INFO: NUM:{}\".format(\n",
    "            round_id, DS.calc_n_cluster(), DS.calc_n_point(), CS.calc_n_cluster(), CS.calc_n_point(), RS.calc_n_point()\n",
    "        ))\n",
    "\n",
    "    def export(self, output_file):\n",
    "        export_file(self.intermediate_records, output_file, output_type='csv')\n",
    "\n",
    "       \n",
    "    \n",
    "        \n",
    "def check_clustering(info, stat, clusters_points):\n",
    "    remaining_points = {}\n",
    "    tmp_clusters_points = copy.deepcopy(clusters_points)\n",
    "    for c, ps in tmp_clusters_points.items():\n",
    "        if len(ps)<=1:\n",
    "            if len(ps)!=0:\n",
    "                remaining_points.update({ps[0]: info.get(c)})\n",
    "            clusters_points.pop(c)\n",
    "            info.pop(c)\n",
    "            stat.pop(c)\n",
    "\n",
    "    return info, stat, clusters_points, remaining_points\n",
    "\n",
    "\n",
    "def assign_set(data, alpha, DS=None, CS=None, cluster_type=''):\n",
    "    if DS and cluster_type==DS.signature:\n",
    "        ds_dim = DS.n_dim\n",
    "        distance = float('inf')\n",
    "        key_to_ds = None\n",
    "        for k, loc in DS.SUM_N.items():\n",
    "            tmp_distance = calc_distance(data[1], loc, DS.STD.get(k), distance_type='mahalanobis')\n",
    "            if tmp_distance < alpha *math.sqrt(ds_dim) and tmp_distance < distance:\n",
    "                distance = tmp_distance\n",
    "                key_to_ds = (k, data[0])\n",
    "\n",
    "        if key_to_ds:\n",
    "            yield tuple((key_to_ds, data[1], False))\n",
    "        else:\n",
    "            yield tuple(((\"-1\", data[0]), data[1], True))\n",
    "\n",
    "    elif CS and cluster_type==CS.signature:\n",
    "        cs_dim = CS.n_dim\n",
    "        distance = float('inf')\n",
    "        key_to_cs = None\n",
    "        for k, loc in CS.SUM_N.items():\n",
    "            tmp_distance = computeDistance(data[1], loc, CS.STD.get(k), distance_type='mahalanobis')\n",
    "            if tmp_distance < alpha *math.sqrt(cs_dim) and tmp_distance < distance:\n",
    "                distance = tmp_distance\n",
    "                key_to_cs = (k, data[0])\n",
    "\n",
    "        if key_to_cs:\n",
    "            yield tuple((key_to_cs, data[1], False))\n",
    "        else:\n",
    "            yield tuple(((\"-1\", data[0]), data[1], True))\n",
    "\n",
    "\n",
    "def merge_CS(alpha, CS):\n",
    "    cs_dim = CS.n_dim\n",
    "    CS_prev = copy.deepcopy(CS)\n",
    "    set_avail = set(list(CS_prev.SUM_N.keys()))\n",
    "    for pair in itertools.combinations(list(CS_prev.SUM_N.keys()), 2):\n",
    "        if pair[0] in set_avail and pair[1] in set_avail:\n",
    "            distance = calc_distance(CS_prev.get_centroid_of_clustuer(pair[0]),\n",
    "                                     CS_prev.get_centroid_of_clustuer(pair[1]),\n",
    "                                     std=CS_prev.get_std_of_clustuer(pair[0]),\n",
    "                                     distance_type='mahalanobis')\n",
    "            if distance < alpha *math.sqrt(cs_dim):\n",
    "                CS.merge(pair[0], pair[1])\n",
    "                set_avail.discard(pair[0])\n",
    "                set_avail.discard(pair[1])\n",
    "\n",
    "\n",
    "def assign_CS2DS(alpha, DS, CS):\n",
    "    ds_dim = DS.n_dim\n",
    "    DS_prev = copy.deepcopy(DS)\n",
    "    CS_prev = copy.deepcopy(CS)\n",
    "    for centroid_cs in CS_prev.SUM_N:\n",
    "        for centroid_ds in DS_prev.SUM_N:\n",
    "            distance = calc_distance(CS_prev.get_centroid_of_clustuer(centroid_cs),\n",
    "                                     DS_prev.get_centroid_of_clustuer(centroid_ds),\n",
    "                                     DS_prev.get_std_of_clustuer(centroid_ds),\n",
    "                                     distance_type='mahalanobis')\n",
    "            if distance < alpha *math.sqrt(ds_dim):\n",
    "                DS.merge(centroid_ds,\n",
    "                         CS_prev.get_sumsq_of_clustuer(centroid_cs),\n",
    "                         CS_prev.get_centroid_of_clustuer(centroid_cs),\n",
    "                         CS_prev.get_points_of_cluster(centroid_cs))\n",
    "                CS.remove_cluster(centroid_cs)\n",
    "                break\n",
    "\n",
    "                \n",
    "def export_clusters_points(DS, CS, RS, output_file):\n",
    "    result = defaultdict()\n",
    "    for c in DS.clusters_points:\n",
    "        [result.setdefault(str(p), int(c[1:])) for p in DS.get_points_of_cluster(c)]\n",
    "        \n",
    "    for c in CS.clusters_points:\n",
    "        [result.setdefault(str(p), -1) for p in CS.get_points_of_cluster(c)]\n",
    "\n",
    "    for k in RS.remaining_set:\n",
    "        result.setdefault(str(k), -1)\n",
    "\n",
    "    export_file(result, output_file, output_type='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # if __name__ == '__main__':\n",
    "# time_start = time.time()\n",
    "\n",
    "# params = sys.argv\n",
    "# input_path, n_cluster = params[1], int(params[2])\n",
    "# out_file1, out_file2 = params[3], params[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path, n_cluster = './test1', 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file1, out_file2 = 'out7', 'out8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -> DS_INFO: C:3 NUM:None | CS_INFO: C:9 NUM:None | RS_INFO: NUM:0\n",
      "2 -> DS_INFO: C:3 NUM:None | CS_INFO: C:7 NUM:None | RS_INFO: NUM:0\n",
      "3 -> DS_INFO: C:3 NUM:None | CS_INFO: C:7 NUM:None | RS_INFO: NUM:0\n",
      "4 -> DS_INFO: C:3 NUM:None | CS_INFO: C:7 NUM:None | RS_INFO: NUM:0\n",
      "5 -> DS_INFO: C:3 NUM:None | CS_INFO: C:0 NUM:None | RS_INFO: NUM:0\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "sc.setLogLevel(\"WARN\")\n",
    "\n",
    "alpha = 3\n",
    "n_times = 3\n",
    "discard_set = DS()\n",
    "compression_set = CS()\n",
    "retained_set = RS()\n",
    "intermediate_records = IntermediateRecords()\n",
    "\n",
    "\n",
    "for i, fp in enumerate(sorted(os.listdir(input_path))):\n",
    "    path = input_path + \"/\" + fp\n",
    "    data = sc.textFile(path).map(lambda r: r.split(\",\")) \\\n",
    "                            .map(lambda kvs: (int(kvs[0]), list(map(eval, kvs[1:]))))\n",
    "\n",
    "    if i==0:\n",
    "        n_data = data.count()\n",
    "        n_sample = 10000 if n_data > 10000 else int(n_data *.1)\n",
    "        data_sample = data.filter(lambda kv: kv[0]<n_sample).collectAsMap()\n",
    "\n",
    "        ds_info, ds_stat, ds_clusters_points = KMeans(n_cluster, max_iter=5).fit(data_sample)\n",
    "\n",
    "        discard_set.init(ds_info, ds_stat, ds_clusters_points)\n",
    "\n",
    "        data_rest = data.filter(lambda kv: kv[0]>=n_sample).collectAsMap()\n",
    "        info, stat, clusters_points = KMeans(n_cluster *n_times, max_iter=3).fit(data_rest)\n",
    "\n",
    "        cs_info, cs_stat, cs_clusters_points, remaining = check_clustering(info, stat, clusters_points)\n",
    "        compression_set.init(cs_info, cs_stat, cs_clusters_points)\n",
    "        retained_set.add(remaining)\n",
    "\n",
    "    else:\n",
    "        data_sets1 = data.flatMap(lambda point: assign_set(point, alpha, DS=discard_set, cluster_type='DS'))\n",
    "\n",
    "        tmp_ds = data_sets1.filter(lambda flag: flag[2] is False) \\\n",
    "                            .map(lambda flag: (flag[0], flag[1]))\n",
    "\n",
    "        tmp_ds_cluster_points = tmp_ds.map(lambda k: k[0]) \\\n",
    "                                      .groupByKey().mapValues(list).collectAsMap()\n",
    "\n",
    "        tmp_ds_data_dict = tmp_ds.map(lambda k: (k[0][1], list(k[1]))).collectAsMap()\n",
    "        discard_set.update_centroid_info(tmp_ds_cluster_points, tmp_ds_data_dict)\n",
    "\n",
    "\n",
    "        data_sets2 = data_sets1.filter(lambda flag: flag[2] is True) \\\n",
    "                                .map(lambda flag: (flag[0][1], flag[1])) \\\n",
    "                                .flatMap(lambda point: assign_set(point, alpha, CS=compression_set, cluster_type='CS'))\n",
    "\n",
    "        tmp_cs = data_sets2.filter(lambda flag: flag[2] is False).map(lambda flag: (flag[0], flag[1]))\n",
    "\n",
    "        tmp_cs_cluster_points = tmp_cs.map(lambda k: k[0]) \\\n",
    "                                      .groupByKey().mapValues(list).collectAsMap()\n",
    "        tmp_cs_data_dict = tmp_cs.map(lambda k: (k[0][1], list(k[1]))).collectAsMap()\n",
    "        compression_set.update_centroid_info(tmp_cs_cluster_points, tmp_cs_data_dict)\n",
    "\n",
    "        remaining_data_dict = data_sets2.filter(lambda flag: flag[2] is True) \\\n",
    "                                        .map(lambda flag: (flag[0][1], flag[1])).collectAsMap()\n",
    "        retained_set.add(remaining_data_dict)\n",
    "\n",
    "\n",
    "        info, stat, clusters_points = KMeans(n_cluster *n_times, max_iter=5).fit(retained_set.remaining_set)\n",
    "        cs_info, cs_stat, cs_clusters_points, remaining2 = check_clustering(info, stat, clusters_points)\n",
    "        compression_set.delta_update(cs_info, cs_stat, cs_clusters_points)\n",
    "        retained_set.gather(remaining2)\n",
    "\n",
    "        merge_CS(alpha, compression_set)\n",
    "\n",
    "    if i+1==len(os.listdir(input_path)):\n",
    "        assign_CS2DS(alpha, discard_set, compression_set)\n",
    "\n",
    "    intermediate_records.save_check_point(i+1, discard_set, compression_set, retained_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time_start' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f677330f7c38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mintermediate_records\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_file2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mexport_clusters_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscard_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretained_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_file1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Duration: %d s.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'time_start' is not defined"
     ]
    }
   ],
   "source": [
    "intermediate_records.export(out_file2)\n",
    "export_clusters_points(discard_set, compression_set, retained_set, out_file1)\n",
    "# print(\"Duration: %d s.\" % (time.time() - time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_clusters_points(DS, CS, RS, output_file):\n",
    "    result = defaultdict()\n",
    "    for c in DS.clusters_points:\n",
    "        [result.setdefault(str(p), int(c[1:])) for p in DS.get_points_of_cluster(c)]\n",
    "        \n",
    "    for c in CS.clusters_points:\n",
    "        [result.setdefault(str(p), -1) for p in CS.get_points_of_cluster(c)]\n",
    "\n",
    "    for k in RS.remaining_set:\n",
    "        result.setdefault(str(k), -1)\n",
    "        \n",
    "    result_sorted = dict(sorted(result.items(), key=lambda kv: int(kv[0])))\n",
    "    export_file(result_sorted, output_file, output_type='json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_clusters_points(discard_set, compression_set, retained_set, out_file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
