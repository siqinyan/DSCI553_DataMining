{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task1: Simulated small data (8.5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import math\n",
    "from itertools import combinations\n",
    "from pyspark import SparkContext\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two CSV files (small1.csv and small2.csv) provided on the Vocareum in your workspace. The small1.csv is just a sample file that you can use to debug your code. For Task1, we will test your code on small2.csv for grading.\n",
    "\n",
    "In this task, you need to build two kinds of market-basket models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = sys.argv\n",
    "# case_number, support = int(params[1]), int(params[2])\n",
    "# input_file_path, output_file_path = params[3], params[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_number, support = 1, 4\n",
    "input_file_path = './small1.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "small = sc.textFile(input_file_path).map(lambda l: l.split(\",\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 1 (4.25 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will calculate the combinations of frequent businesses (as singletons, pairs, triples, etc.) that are qualified as “frequent” given a support threshold. You need to create a basket for each user containing the business ids reviewed by this user. If a business was reviewed more than once by a reviewer, we consider this product was rated only once. More specifically, the business ids within each basket are unique. The generated baskets are similar to:\n",
    "\n",
    "user1: [business11, business12, business13, ...]\n",
    "\n",
    "user2: [business21, business22, business23, ...]\n",
    "\n",
    "user3: [business31, business32, business33, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# header = small.top(1)[0]\n",
    "header = small.first()\n",
    "if case_number==1:\n",
    "    data = small.filter(lambda l: l!=header) \\\n",
    "                .map(lambda u_b: (u_b[0], u_b[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2 (4.25 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will calculate the combinations of frequent users (as singletons, pairs, triples, etc.) that are qualified as “frequent” given a support threshold. You need to create a basket for each business containing the user ids that commented on this business. Similar to case 1, the user ids within each basket are unique. The generated baskets are similar to:\n",
    "\n",
    "business1: [user11, user12, user13, ...] \n",
    "\n",
    "business2: [user21, user22, user23, ...] \n",
    "\n",
    "business3: [user31, user32, user33, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if case_number==2:\n",
    "    data = small.filter(lambda l: l!=header) \\\n",
    "            .map(lambda u_b: (u_b[1], u_b[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', '100'), ('1', '98')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['100', '101', '102', '98'],\n",
       " ['101', '102', '103', '97', '99'],\n",
       " ['102', '103', '104', '97', '98', '99'],\n",
       " ['97', '98', '99'],\n",
       " ['97', '98']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "participants = data.groupByKey().map(lambda u_b: sorted(list(set(u_b[1]))))\n",
    "participants.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_data = participants.count()\n",
    "len_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counts_singleton(partitions):\n",
    "    counts = {}\n",
    "    for pars in partitions:\n",
    "        for p in pars:\n",
    "            if p in counts: counts[p] += 1\n",
    "            else: counts[p] = 1\n",
    "    return counts\n",
    "\n",
    "def frequent_items(counts, threshold):\n",
    "    freq_items = dict(filter(lambda p_c: p_c[1]>=threshold, counts.items()))\n",
    "    return sorted(freq_items.keys())\n",
    "\n",
    "def counts_items(partitions, freq_items, n_items):\n",
    "    counts = {}\n",
    "    for pars in partitions:\n",
    "        pars = sorted(set(pars) & set(freq_items))\n",
    "        for p in combinations(pars, n_items):\n",
    "            if p in counts: counts[p] += 1\n",
    "            else: counts[p] = 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_candidates(participants, support, len_data):\n",
    "    candidates = []\n",
    "    partitions = list(participants)\n",
    "    ps = math.ceil(support * len(partitions) / len_data)\n",
    "    \n",
    "    count_singleton = counts_singleton(partitions)\n",
    "    freq_singleton = frequent_items(count_singleton, ps)\n",
    "    candidates.append([(i,) for i in freq_singleton])\n",
    "\n",
    "    n_items = 2\n",
    "    freq_n_items, freq_n0_items = [None], freq_singleton\n",
    "    while freq_n_items:\n",
    "        count_items = counts_items(partitions, freq_n0_items, n_items)\n",
    "        freq_n_items = frequent_items(count_items, ps)\n",
    "        candidates.append(freq_n_items)\n",
    "        freq_n0_items = set()\n",
    "        for i in freq_n_items:\n",
    "            freq_n0_items = freq_n0_items | set(i)\n",
    "        n_items += 1\n",
    "        \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('100',), ('101',), ('102',), ('97',), ('98',), ('99',)],\n",
       " [('100', '101'),\n",
       "  ('100', '98'),\n",
       "  ('101', '102'),\n",
       "  ('101', '98'),\n",
       "  ('102', '97'),\n",
       "  ('102', '98'),\n",
       "  ('97', '98'),\n",
       "  ('97', '99'),\n",
       "  ('98', '99')],\n",
       " [('100', '101', '98')],\n",
       " [],\n",
       " [('100',), ('101',), ('102',), ('103',), ('105',), ('97',), ('98',), ('99',)],\n",
       " [('100', '101'),\n",
       "  ('100', '99'),\n",
       "  ('101', '102'),\n",
       "  ('101', '97'),\n",
       "  ('101', '99'),\n",
       "  ('102', '103'),\n",
       "  ('102', '105'),\n",
       "  ('102', '98'),\n",
       "  ('102', '99'),\n",
       "  ('103', '105'),\n",
       "  ('103', '98'),\n",
       "  ('103', '99'),\n",
       "  ('105', '98'),\n",
       "  ('105', '99'),\n",
       "  ('97', '98'),\n",
       "  ('97', '99'),\n",
       "  ('98', '99')],\n",
       " [('100', '101', '99'),\n",
       "  ('101', '97', '99'),\n",
       "  ('102', '103', '105'),\n",
       "  ('102', '103', '98'),\n",
       "  ('102', '103', '99'),\n",
       "  ('102', '105', '98'),\n",
       "  ('102', '105', '99'),\n",
       "  ('102', '98', '99'),\n",
       "  ('103', '105', '98'),\n",
       "  ('103', '105', '99'),\n",
       "  ('103', '98', '99'),\n",
       "  ('105', '98', '99'),\n",
       "  ('97', '98', '99')],\n",
       " [('102', '103', '105', '98'),\n",
       "  ('102', '103', '105', '99'),\n",
       "  ('102', '103', '98', '99'),\n",
       "  ('102', '105', '98', '99'),\n",
       "  ('103', '105', '98', '99')],\n",
       " [('102', '103', '105', '98', '99')],\n",
       " []]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates = participants.mapPartitions(lambda b: find_candidates(b, support, len_data))\n",
    "candidates.collect()\n",
    "# candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('100',),\n",
       " ('101',),\n",
       " ('102',),\n",
       " ('103',),\n",
       " ('105',),\n",
       " ('97',),\n",
       " ('98',),\n",
       " ('99',),\n",
       " ('100', '101'),\n",
       " ('100', '98'),\n",
       " ('100', '99'),\n",
       " ('101', '102'),\n",
       " ('101', '97'),\n",
       " ('101', '98'),\n",
       " ('101', '99'),\n",
       " ('102', '103'),\n",
       " ('102', '105'),\n",
       " ('102', '97'),\n",
       " ('102', '98'),\n",
       " ('102', '99'),\n",
       " ('103', '105'),\n",
       " ('103', '98'),\n",
       " ('103', '99'),\n",
       " ('105', '98'),\n",
       " ('105', '99'),\n",
       " ('97', '98'),\n",
       " ('97', '99'),\n",
       " ('98', '99'),\n",
       " ('100', '101', '98'),\n",
       " ('100', '101', '99'),\n",
       " ('101', '97', '99'),\n",
       " ('102', '103', '105'),\n",
       " ('102', '103', '98'),\n",
       " ('102', '103', '99'),\n",
       " ('102', '105', '98'),\n",
       " ('102', '105', '99'),\n",
       " ('102', '98', '99'),\n",
       " ('103', '105', '98'),\n",
       " ('103', '105', '99'),\n",
       " ('103', '98', '99'),\n",
       " ('105', '98', '99'),\n",
       " ('97', '98', '99'),\n",
       " ('102', '103', '105', '98'),\n",
       " ('102', '103', '105', '99'),\n",
       " ('102', '103', '98', '99'),\n",
       " ('102', '105', '98', '99'),\n",
       " ('103', '105', '98', '99'),\n",
       " ('102', '103', '105', '98', '99')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidates = participants.mapPartitions(lambda b: find_candidates(b, support, len_data)) \\\n",
    "                .flatMap(lambda x: x).distinct() \\\n",
    "                .sortBy(lambda x: (len(x), x))\n",
    "candidates = candidates.collect()\n",
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_frequent(partition, candidates):\n",
    "    freq = {}\n",
    "    for pars in partition:\n",
    "        for i in candidates:\n",
    "            if set(i).issubset(pars):\n",
    "                if i in freq:\n",
    "                    freq[i] += 1\n",
    "                else:\n",
    "                    freq[i] = 1\n",
    "                    \n",
    "    freq = [(k, v) for k, v in freq.items()]\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('100',),\n",
       " ('101',),\n",
       " ('102',),\n",
       " ('103',),\n",
       " ('97',),\n",
       " ('98',),\n",
       " ('99',),\n",
       " ('100', '101'),\n",
       " ('100', '98'),\n",
       " ('101', '102'),\n",
       " ('101', '97'),\n",
       " ('101', '98'),\n",
       " ('101', '99'),\n",
       " ('102', '103'),\n",
       " ('102', '97'),\n",
       " ('102', '98'),\n",
       " ('102', '99'),\n",
       " ('103', '99'),\n",
       " ('97', '98'),\n",
       " ('97', '99'),\n",
       " ('98', '99'),\n",
       " ('100', '101', '98'),\n",
       " ('101', '97', '99'),\n",
       " ('102', '103', '99'),\n",
       " ('97', '98', '99')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = participants.mapPartitions(lambda b: find_frequent(b, candidates)) \\\n",
    "                    .reduceByKey(add) \\\n",
    "                    .filter(lambda f: f[1]>=support).map(lambda f: f[0]) \\\n",
    "                    .sortBy(lambda x: (len(x), x))\n",
    "freq = freq.collect()\n",
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"('100'\", ')']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(('100',)).split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_form(data):\n",
    "    output = ''\n",
    "    len_cur = 1\n",
    "    for i in data:\n",
    "        if len(i)==1:\n",
    "            output += f'{str(i).split(',')[0]}),'\n",
    "        elif len(i)==len_cur:\n",
    "            output += f'{str(i)},'\n",
    "        else:\n",
    "            output = output[:-1]\n",
    "            output += f'\\n\\n{str(i)},'\n",
    "            len_cur = len(i)\n",
    "    return output[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file_path, 'w+') as f:\n",
    "    f.write('Candidates:\\n' + output_form(candidates) + '\\n\\n' + 'Frequent Itemsets:\\n' + output_form(freq))\n",
    "\n",
    "time_end = time.time()\n",
    "print(f'Duration: {time_end - time_start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Real-world data set -Yelp data- (4.0 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In task2, you will explore the Yelp dataset to find the frequent business sets (only case 1). You will jointly\n",
    "use the business.json and review.json to generate the input user-business CSV file yourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to generate a sample dataset from business.json and review.json with the following steps:\n",
    "1. The state of the business you need is Nevada, i.e., filtering ‘state’== ‘NV’.\n",
    "2. Select “user_id” and “business_id” from review.json whose “business_id” is from Nevada. Each line in the CSV file would be “user_id1, business_id1”.\n",
    "3. The header of CSV file should be “user_id,business_id”\n",
    "\n",
    "You need to save the dataset in CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/antheayang/Desktop/llleArn/DSCI553/HW2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "# os.chdir()\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/antheayang/Desktop/llleArn/DSCI553'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/Users/antheayang/Desktop/llleArn/DSCI553')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_path = './HW1/review.json'\n",
    "business_path = './HW1/business.json'\n",
    "state = 'NV'\n",
    "output_path = './HW2/user_business.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "review = sc.textFile(review_path).map(lambda r: json.loads(r)) \n",
    "business = sc.textFile(business_path).map(lambda r: json.loads(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "business_need = business.map(lambda r: (r['business_id'], r['state'])) \\\n",
    "                        .filter(lambda r: r[1]==state).map(lambda r: r[0])\n",
    "business_need = business_need.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = review.map(lambda r: (r['user_id'], r['business_id'])) \\\n",
    "               .filter(lambda r: r[1] in business_need)\n",
    "output = output.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_path, 'w+') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['user_id', 'business_id'])\n",
    "    for r in output:\n",
    "        writer.writerow(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Apply SON algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The requirements for task 2 are similar to task 1. However, you will test your implementation with the large dataset you just generated. For this purpose, you need to report the total execution time. For this execution time, we take into account also the time from reading the file till writing the results to the output file. You are asked to find the frequent business sets (only case 1) from the file you just generated. The following are the steps you need to do:\n",
    "1. Reading the user_business CSV file in to RDD and then build the case 1 market-basket model; \n",
    "2. Find out qualified users who reviewed more than k businesses. (k is the filter threshold);\n",
    "3. Apply the SON algorithm code to the filtered market-basket model;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "from itertools import combinations\n",
    "from pyspark import SparkContext\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counts_singleton(partitions):\n",
    "    counts = {}\n",
    "    for pars in partitions:\n",
    "        for p in pars:\n",
    "            if p in counts: counts[p] += 1\n",
    "            else: counts[p] = 1\n",
    "    return counts\n",
    "\n",
    "def frequent_items(counts, threshold):\n",
    "    freq_items = dict(filter(lambda p_c: p_c[1]>=threshold, counts.items()))\n",
    "    return sorted(freq_items.keys())\n",
    "\n",
    "def counts_items(partitions, freq_items, n_items):\n",
    "    counts = {}\n",
    "    for pars in partitions:\n",
    "        pars = sorted(set(pars) & set(freq_items))\n",
    "        for p in combinations(pars, n_items):\n",
    "            if p in counts: counts[p] += 1\n",
    "            else: counts[p] = 1\n",
    "    return counts\n",
    "\n",
    "def find_candidates(participants, support, len_data):\n",
    "    candidates = []\n",
    "    partitions = list(participants)\n",
    "    ps = math.ceil(support * len(partitions) / len_data)\n",
    "\n",
    "    count_singleton = counts_singleton(partitions)\n",
    "    freq_singleton = frequent_items(count_singleton, ps)\n",
    "    candidates.append([(i,) for i in freq_singleton])\n",
    "\n",
    "    n_items = 2\n",
    "    freq_n_items, freq_n0_items = [None], freq_singleton\n",
    "    while freq_n_items:\n",
    "        count_items = counts_items(partitions, freq_n0_items, n_items)\n",
    "        freq_n_items = frequent_items(count_items, ps)\n",
    "        candidates.append(freq_n_items)\n",
    "        freq_n0_items = set()\n",
    "        for i in freq_n_items:\n",
    "            freq_n0_items = freq_n0_items | set(i)\n",
    "        n_items += 1\n",
    "    return candidates\n",
    "\n",
    "\n",
    "def find_frequent(partition, candidates):\n",
    "    freq = {}\n",
    "    for pars in partition:\n",
    "        for i in candidates:\n",
    "            if set(i).issubset(pars):\n",
    "                if i in freq:\n",
    "                    freq[i] += 1\n",
    "                else:\n",
    "                    freq[i] = 1\n",
    "\n",
    "    freq = [(k, v) for k, v in freq.items()]\n",
    "    return freq\n",
    "\n",
    "\n",
    "def output_form(data):\n",
    "    output = ''\n",
    "    len_cur = 1\n",
    "    for i in data:\n",
    "        if len(i)==1:\n",
    "            output += str(i).split(',')[0] + '),'\n",
    "        elif len(i)==len_cur:\n",
    "            output += str(i) + ','\n",
    "        else:\n",
    "            output = output[:-1]\n",
    "            output += '\\n\\n' + str(i) + ','\n",
    "            len_cur = len(i)\n",
    "    return output[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 11.842086791992188\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "# params = sys.argv\n",
    "# filter_threshold, support = int(params[1]), int(params[2])\n",
    "# input_file_path, output_file_path = params[3], params[4]\n",
    "filter_threshold, support = 70, 50\n",
    "# input_file_path, output_file_path = './ub.csv', './test.csv'\n",
    "input_file_path, output_file_path = './user_business.csv', './test.csv'\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "user_business = sc.textFile(input_file_path).map(lambda l: l.split(\",\"))\n",
    "header = user_business.first()\n",
    "data = user_business.filter(lambda l: l!=header) \\\n",
    "                    .map(lambda u_b: (u_b[0], u_b[1]))\n",
    "\n",
    "participants = data.groupByKey().map(lambda u_b: sorted(list(set(u_b[1])))) \\\n",
    "                    .filter(lambda b: len(b)>filter_threshold)\n",
    "len_data = participants.count()\n",
    "\n",
    "candidates = participants.mapPartitions(lambda b: find_candidates(b, support, len_data)) \\\n",
    "                        .flatMap(lambda x: x).distinct() \\\n",
    "                        .sortBy(lambda x: (len(x), x))\n",
    "candidates = candidates.collect()    \n",
    "\n",
    "freq = participants.mapPartitions(lambda b: find_frequent(b, candidates)) \\\n",
    "                .reduceByKey(add) \\\n",
    "                .filter(lambda f: f[1]>=support).map(lambda f: f[0]) \\\n",
    "                .sortBy(lambda f: (len(f), f))\n",
    "freq = freq.collect()\n",
    "\n",
    "\n",
    "with open(output_file_path, 'w+') as f:\n",
    "    f.write('Candidates:\\n' + output_form(candidates) + '\\n\\n' + 'Frequent Itemsets:\\n' + output_form(freq))\n",
    "\n",
    "time_end = time.time()\n",
    "print(f'Duration: {time_end - time_start}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
