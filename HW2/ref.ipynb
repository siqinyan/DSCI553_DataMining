{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import copy\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "from functools import reduce\n",
    "from itertools import combinations\n",
    "from operator import add\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# TODO change the number of bucket will have different result\n",
    "BUCKET_NUMBER = 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_func(combination):\n",
    "    result = sum(map(lambda x: int(x), list(combination)))\n",
    "    return result % BUCKET_NUMBER\n",
    "\n",
    "\n",
    "def check_bitmap(combination, bitmap):\n",
    "    \"\"\"\n",
    "    check if its hash result in bitmap\n",
    "    :param combination:\n",
    "    :param bitmap:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return bitmap[hash_func(combination)]\n",
    "\n",
    "\n",
    "def wrapper(singleton_list):\n",
    "    \"\"\"\n",
    "    reformat str item into tuple\n",
    "    :param singleton_list:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return [tuple(item.split(\",\")) for item in singleton_list]\n",
    "\n",
    "def gen_ps_threshold(partition, support, whole_length):\n",
    "    \"\"\"\n",
    "    generate each partition's support threshold\n",
    "    :param partition:\n",
    "    :param support:\n",
    "    :param whole_length: the original rdd's size\n",
    "    :return: support threshold in this partition\n",
    "    \"\"\"\n",
    "    partition = copy.deepcopy(list(partition))\n",
    "    return math.ceil(support * len(list(partition)) / whole_length), partition\n",
    "\n",
    "\n",
    "def reformat(itemset_data):\n",
    "    \"\"\"\n",
    "    reformat pairs which length is 1 ('100',), -> ('100'),\n",
    "    and a line break after each subset who has a same length\n",
    "    :param itemset_data: a list of paris (singletons, pairs, triples, etc.)\n",
    "    :return: a formatted str\n",
    "    \"\"\"\n",
    "    temp_index = 1\n",
    "    result_str = \"\"\n",
    "    for pair in itemset_data:\n",
    "        if len(pair) == 1:\n",
    "            result_str += str(\"(\" + str(pair)[1:-2] + \"),\")\n",
    "\n",
    "        elif len(pair) != temp_index:\n",
    "            result_str = result_str[:-1]\n",
    "            result_str += \"\\n\\n\"\n",
    "            temp_index = len(pair)\n",
    "            result_str += (str(pair) + \",\")\n",
    "        else:\n",
    "            result_str += (str(pair) + \",\")\n",
    "\n",
    "    return result_str[:-1]\n",
    "\n",
    "\n",
    "def export_2_file(candidate_data, frequent_data, file_path):\n",
    "    with open(file_path, 'w+') as output_file:\n",
    "        str_result = 'Candidates:\\n' + reformat(candidate_data) + '\\n\\n' \\\n",
    "                     + 'Frequent Itemsets:\\n' + reformat(frequent_data)\n",
    "        output_file.write(str_result)\n",
    "        output_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_singleton_and_bitmap(baskets, support):\n",
    "    \"\"\"\n",
    "    first phrase of PCY algorithm\n",
    "    :param baskets:\n",
    "    :param support:\n",
    "    :return: frequent_singleton: a list of sorted frequent singleton =>  ['100', '101', '102'...\n",
    "                bitmap: a boolean list => [True, False, True ...\n",
    "    \"\"\"\n",
    "    bitmap = [0 for _ in range(BUCKET_NUMBER)]\n",
    "    temp_counter = collections.defaultdict(list)\n",
    "    for basket in baskets:\n",
    "        # find frequent singleton\n",
    "        for item in basket:\n",
    "            temp_counter[item].append(1)\n",
    "\n",
    "        # find frequent bucket\n",
    "        for pair in combinations(basket, 2):\n",
    "            key = hash_func(pair)\n",
    "            bitmap[key] = (bitmap[key] + 1)\n",
    "\n",
    "    filtered_dict = dict(filter(lambda elem: len(elem[1]) >= support, temp_counter.items()))\n",
    "    frequent_singleton = sorted(list(filtered_dict.keys()))\n",
    "    bitmap = list(map(lambda value: True if value >= support else False, bitmap))\n",
    "#     bitmap = list(map(lambda value: value if value >= support else False, bitmap))\n",
    "\n",
    "    return frequent_singleton, bitmap\n",
    "\n",
    "\n",
    "def shrink_basket(basket, frequent_singleton):\n",
    "    \"\"\"\n",
    "    we dont need to compute basket_item which is not frequent_single,\n",
    "    basically we can return the interaction about this two set\n",
    "    :param basket:\n",
    "    :param frequent_singleton:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return sorted(list(set(basket).intersection(set(frequent_singleton))))\n",
    "\n",
    "\n",
    "def cmp(pair1, pair2):\n",
    "    return True if pair1[:-1] == pair2[:-1] else False\n",
    "\n",
    "\n",
    "def gen_permutation(combination_list):\n",
    "    \"\"\"\n",
    "\n",
    "    :param combination_list: assume every pair in combination_list is n\n",
    "    :return: a list of candidate pair whose shape is n + 1\n",
    "    \"\"\"\n",
    "    if combination_list is not None and len(combination_list) > 0:\n",
    "        size = len(combination_list[0])\n",
    "        permutation_list = list()\n",
    "        for index, front_pair in enumerate(combination_list[:-1]):\n",
    "            for back_pair in combination_list[index + 1:]:\n",
    "                if cmp(front_pair, back_pair):\n",
    "                    combination = tuple(sorted(list(set(front_pair).union(set(back_pair)))))\n",
    "                    temp_pair = list()\n",
    "                    for pair in combinations(combination, size):\n",
    "                        temp_pair.append(pair)\n",
    "                    if set(temp_pair).issubset(set(combination_list)):\n",
    "                        permutation_list.append(combination)\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        return permutation_list\n",
    "\n",
    "\n",
    "def find_candidate_itemset(data_baskets, original_support, whole_length):\n",
    "    \"\"\"\n",
    "    using PCY to find frequent itemset from subset basket\n",
    "    :param data_baskets: subset baskets\n",
    "    :param original_support:\n",
    "    :param whole_length:\n",
    "    :return: all candidate itemsets list\n",
    "    \"\"\"\n",
    "\n",
    "    # compute support threshold in subset baskets\n",
    "    support, data_baskets = gen_ps_threshold(data_baskets, original_support, whole_length)\n",
    "    baskets_list = list(data_baskets)\n",
    "    # print(\"baskets_list -> \", baskets_list)\n",
    "    all_candidate_dict = collections.defaultdict(list)\n",
    "    # first phrase of PCY algorithm, acquiring frequent_singleton and bitmap\n",
    "    frequent_singleton, bitmap = init_singleton_and_bitmap(baskets_list, support)    \n",
    "    index = 1\n",
    "    candidate_list = frequent_singleton\n",
    "    all_candidate_dict[str(index)] = wrapper(frequent_singleton)\n",
    "\n",
    "    # the second phrase, third phrase .... until the candidate list is empty\n",
    "    while None is not candidate_list and len(candidate_list) > 0:\n",
    "        index += 1\n",
    "        temp_counter = collections.defaultdict(list)\n",
    "        for basket in baskets_list:\n",
    "            # we dont need to compute basket_item which is not frequent_single\n",
    "            basket = shrink_basket(basket, frequent_singleton)\n",
    "            if len(basket) >= index:\n",
    "                if index == 2:\n",
    "                    for pair in combinations(basket, index):\n",
    "                        if check_bitmap(pair, bitmap):\n",
    "                            # if check_proper_subset(pair, candidate_list):\n",
    "                            # this is always true, since you have filter the basket before\n",
    "                            temp_counter[pair].append(1)\n",
    "\n",
    "                if index >= 3:\n",
    "                    for candidate_item in candidate_list:\n",
    "                        if set(candidate_item).issubset(set(basket)):\n",
    "                            temp_counter[candidate_item].append(1)\n",
    "\n",
    "        # filter the temp_counter\n",
    "        filtered_dict = dict(filter(lambda elem: len(elem[1]) >= support, temp_counter.items()))\n",
    "        # generate new candidate list\n",
    "        candidate_list = gen_permutation(sorted(list(filtered_dict.keys())))\n",
    "        if len(filtered_dict) == 0:\n",
    "            break\n",
    "        all_candidate_dict[str(index)] = list(filtered_dict.keys())\n",
    "\n",
    "    yield reduce(lambda val1, val2: val1 + val2, all_candidate_dict.values())\n",
    "\n",
    "\n",
    "def count_frequent_itemset(data_baskets, candidate_pairs):\n",
    "    \"\"\"\n",
    "    count how many time each candidate item occurred in the sub baskets\n",
    "    :param data_baskets: flatted baskets\n",
    "    :param candidate_pairs: all candidate pairs\n",
    "    :return: C, v\n",
    "    \"\"\"\n",
    "    temp_counter = collections.defaultdict(list)\n",
    "    for pairs in candidate_pairs:\n",
    "        if set(pairs).issubset(set(data_baskets)):\n",
    "            temp_counter[pairs].append(1)\n",
    "\n",
    "    yield [tuple((key, sum(value))) for key, value in temp_counter.items()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(data_baskets, original_support, whole_length):\n",
    "    support, data_baskets = gen_ps_threshold(data_baskets, original_support, whole_length)\n",
    "    baskets_list = list(data_baskets)\n",
    "    # print(\"baskets_list -> \", baskets_list)\n",
    "    all_candidate_dict = collections.defaultdict(list)\n",
    "    # first phrase of PCY algorithm, acquiring frequent_singleton and bitmap\n",
    "    frequent_singleton, bitmap = init_singleton_and_bitmap(baskets_list, support)    \n",
    "    index = 1\n",
    "    candidate_list = frequent_singleton\n",
    "    all_candidate_dict[str(index)] = wrapper(frequent_singleton)\n",
    "\n",
    "    # the second phrase, third phrase .... until the candidate list is empty\n",
    "#     while None is not candidate_list and len(candidate_list) > 0:\n",
    "    index += 1\n",
    "    temp_counter = collections.defaultdict(list)\n",
    "    basket_shrinked = []\n",
    "    for basket in baskets_list:\n",
    "        # we dont need to compute basket_item which is not frequent_single\n",
    "        basket_shrinked.append(shrink_basket(basket, frequent_singleton))\n",
    "    return basket_shrinked\n",
    "#     return frequent_singleton, bitmap\n",
    "\n",
    "tmp = basket_rdd.mapPartitions(\n",
    "    lambda partition: func(\n",
    "        data_baskets=partition,\n",
    "        original_support=int(support_threshold),\n",
    "        whole_length=whole_data_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['100', '101', '102', '98'],\n",
       " ['101', '102', '97', '99'],\n",
       " ['102', '97', '98', '99'],\n",
       " ['97', '98', '99'],\n",
       " ['97', '98'],\n",
       " ['100', '101', '102', '98'],\n",
       " ['97'],\n",
       " ['100', '101', '98', '99'],\n",
       " ['97', '99'],\n",
       " ['102', '97', '98'],\n",
       " ['100', '101', '97', '99'],\n",
       " ['102', '103', '105', '97', '98', '99'],\n",
       " ['97', '98'],\n",
       " ['101', '102'],\n",
       " ['101', '97', '99'],\n",
       " ['97', '98', '99'],\n",
       " ['100', '101', '102', '103', '105', '98', '99'],\n",
       " ['101', '97', '99'],\n",
       " ['97', '98', '99']]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "case_number = \"1\"  # 1 for Case 1 and 2 for Case 2\n",
    "support_threshold = \"4\"\n",
    "input_csv_path = './small1.csv'\n",
    "# output_file_path = \"../out/output6.txt\"\n",
    "\n",
    "partition_number = 2\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "raw_rdd = sc.textFile(input_csv_path, partition_number)\n",
    "# skip the first row => csv header\n",
    "header = raw_rdd.first()\n",
    "data_rdd = raw_rdd.filter(lambda line: line != header)\n",
    "whole_data_size = None\n",
    "basket_rdd = None\n",
    "\n",
    "if 1 == int(case_number):\n",
    "    # frequent businesses market-basket model\n",
    "    basket_rdd = data_rdd.map(lambda line: (line.split(',')[0], line.split(',')[1])) \\\n",
    "        .groupByKey().map(lambda user_items: (user_items[0], sorted(list(set(list(user_items[1])))))) \\\n",
    "        .map(lambda item_users: item_users[1])\n",
    "\n",
    "elif 2 == int(case_number):\n",
    "    # frequent user market-basket model\n",
    "    basket_rdd = data_rdd.map(lambda line: (line.split(',')[1], line.split(',')[0])) \\\n",
    "        .groupByKey().map(lambda item_users: (item_users[0], sorted(list(set(list(item_users[1])))))) \\\n",
    "        .map(lambda item_users: item_users[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('100',), ('101',), ('102',), ('103',), ('105',), ('97',), ('98',), ('99',), ('100', '101'), ('100', '98'), ('100', '99'), ('101', '102'), ('101', '97'), ('101', '98'), ('101', '99'), ('102', '103'), ('102', '105'), ('102', '97'), ('102', '98'), ('102', '99'), ('103', '105'), ('103', '98'), ('103', '99'), ('105', '98'), ('105', '99'), ('97', '98'), ('97', '99'), ('98', '99'), ('100', '101', '98'), ('100', '101', '99'), ('101', '97', '99'), ('102', '103', '105'), ('102', '103', '98'), ('102', '103', '99'), ('102', '105', '98'), ('102', '105', '99'), ('102', '98', '99'), ('103', '105', '98'), ('103', '105', '99'), ('103', '98', '99'), ('105', '98', '99'), ('97', '98', '99'), ('102', '103', '105', '98'), ('102', '103', '105', '99'), ('102', '103', '98', '99'), ('102', '105', '98', '99'), ('103', '105', '98', '99'), ('102', '103', '105', '98', '99')]\n"
     ]
    }
   ],
   "source": [
    "# implement SON Algorithm\n",
    "# phrase 1 subset of data -> (F,1) -> distinct -> sort\n",
    "whole_data_size = basket_rdd.count()\n",
    "\n",
    "candidate_itemset = basket_rdd.mapPartitions(\n",
    "    lambda partition: find_candidate_itemset(\n",
    "        data_baskets=partition,\n",
    "        original_support=int(support_threshold),\n",
    "        whole_length=whole_data_size)) \\\n",
    "    .flatMap(lambda pairs: pairs).distinct() \\\n",
    "    .sortBy(lambda pairs: (len(pairs), pairs)).collect()\n",
    "print(candidate_itemset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('100',), ('101',), ('102',), ('103',), ('97',), ('98',), ('99',), ('100', '101'), ('100', '98'), ('101', '102'), ('101', '97'), ('101', '98'), ('101', '99'), ('102', '103'), ('102', '97'), ('102', '98'), ('102', '99'), ('103', '99'), ('97', '98'), ('97', '99'), ('98', '99'), ('100', '101', '98'), ('101', '97', '99'), ('102', '103', '99'), ('97', '98', '99')]\n"
     ]
    }
   ],
   "source": [
    "# phrase 2 subset of data + candidate_pairs -> (C, v) -> reduceByKey(add) -> filter\n",
    "frequent_itemset = basket_rdd.flatMap(\n",
    "    lambda partition: count_frequent_itemset(\n",
    "        data_baskets=partition,\n",
    "        candidate_pairs=candidate_itemset)) \\\n",
    "    .flatMap(lambda pairs: pairs).reduceByKey(add) \\\n",
    "    .filter(lambda pair_count: pair_count[1] >= int(support_threshold)) \\\n",
    "    .map(lambda pair_count: pair_count[0]) \\\n",
    "    .sortBy(lambda pairs: (len(pairs), pairs)).collect()\n",
    "print(frequent_itemset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export_2_file(candidate_data=candidate_itemset,\n",
    "#               frequent_data=frequent_itemset,\n",
    "#               file_path=output_file_path)\n",
    "\n",
    "print(\"Duration: %d s.\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>18</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>18</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>19</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>19</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>19</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  business_id\n",
       "0         1          100\n",
       "1         1           98\n",
       "2         1          101\n",
       "3         1          102\n",
       "4         2          101\n",
       "..      ...          ...\n",
       "69       18           99\n",
       "70       18           97\n",
       "71       19          102\n",
       "72       19           97\n",
       "73       19           98\n",
       "\n",
       "[74 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(input_csv_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user_id\n",
       "business_id         \n",
       "97                14\n",
       "98                12\n",
       "99                12\n",
       "100                5\n",
       "101                9\n",
       "102                8\n",
       "103                4\n",
       "104                1\n",
       "105                3\n",
       "106                2\n",
       "107                2\n",
       "108                2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('business_id').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
